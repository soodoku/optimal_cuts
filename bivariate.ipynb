{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f0773c6-f94f-4c07-8b05-efc3a63aaf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def optimal_bins_dp(x, y, k):\n",
    "    N = len(x)\n",
    "    # Precompute prefix sums for fast MSE calc\n",
    "    prefix_sum = np.cumsum(np.concatenate([[0], y]))\n",
    "    prefix_sq = np.cumsum(np.concatenate([[0], y**2]))\n",
    "    # dp[b][i] = minimum SSE (sum of squared errors) for binning first i points into b bins\n",
    "    dp = np.full((k+1, N+1), np.inf)\n",
    "    prev_cut = np.zeros((k+1, N+1), dtype=int)  # to store optimal cut index\n",
    "    dp[0][0] = 0  # 0 bins for 0 points = 0 error\n",
    "\n",
    "    # Base case: 1 bin for first i points = error of one segment [1..i]\n",
    "    for i in range(1, N+1):\n",
    "        # SSE for segment 1..i:\n",
    "        sum_y = prefix_sum[i] - prefix_sum[0]\n",
    "        sum_y2 = prefix_sq[i] - prefix_sq[0]\n",
    "        seg_len = i\n",
    "        dp[1][i] = sum_y2 - (sum_y**2)/seg_len\n",
    "\n",
    "    # Fill DP for b = 2..k\n",
    "    for b in range(2, k+1):\n",
    "        for i in range(b, N+1):  # need at least b points for b bins\n",
    "            # try placing the (b-th) bin start at j (meaning previous cut at j)\n",
    "            for j in range(b-1, i):\n",
    "                # error = dp[b-1][j] + SSE of segment (j+1 .. i)\n",
    "                sum_y = prefix_sum[i] - prefix_sum[j]\n",
    "                sum_y2 = prefix_sq[i] - prefix_sq[j]\n",
    "                seg_len = i - j\n",
    "                sse_segment = sum_y2 - (sum_y**2)/seg_len\n",
    "                total_sse = dp[b-1][j] + sse_segment\n",
    "                if total_sse < dp[b][i]:\n",
    "                    dp[b][i] = total_sse\n",
    "                    prev_cut[b][i] = j\n",
    "    # Backtrack to find cut indices\n",
    "    cuts = []\n",
    "    b, i = k, N\n",
    "    while b > 1:\n",
    "        j = prev_cut[b][i]\n",
    "        cuts.append(j)\n",
    "        i, b = j, b-1\n",
    "    cuts = sorted(cuts)  # indices where bins end\n",
    "    # Convert cut indices to bin edge values (midpoints between x[c] and x[c+1])\n",
    "    edges = [ (x[c-1] + x[c])/2 for c in cuts ]\n",
    "    return edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a19ffab1-3506-4204-891f-33272304d543",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m     edges \u001b[38;5;241m=\u001b[39m [(centers[i] \u001b[38;5;241m+\u001b[39m centers[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(centers)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m edges\n\u001b[0;32m---> 11\u001b[0m edges \u001b[38;5;241m=\u001b[39m kmeans_binning(\u001b[43mx_values\u001b[49m, y_target, n_bins\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mK-means bin edges:\u001b[39m\u001b[38;5;124m\"\u001b[39m, edges)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_values' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_binning(x, y, n_bins):\n",
    "    km = KMeans(n_clusters=n_bins, n_init=10, random_state=0)\n",
    "    labels = km.fit_predict(x.reshape(-1,1))\n",
    "    centers = np.sort(km.cluster_centers_.flatten())\n",
    "    # Compute midpoints between sorted cluster centers as edges\n",
    "    edges = [(centers[i] + centers[i+1]) / 2 for i in range(len(centers)-1)]\n",
    "    return edges\n",
    "\n",
    "edges = kmeans_binning(x_values, y_target, n_bins=5)\n",
    "print(\"K-means bin edges:\", edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b739ba6-0f87-4905-bd4f-a463ac588ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "def isotonic_binning(x, y, n_bins):\n",
    "    iso = IsotonicRegression(increasing=True).fit(x, y)\n",
    "    y_iso_pred = iso.predict(x)           # isotonic fitted values (monotonic sequence)\n",
    "    # Cluster the isotonic predictions into k groups (e.g., using k-means in value-space)\n",
    "    centers, labels = KMeans(n_clusters=n_bins, n_init=10, random_state=0) \\\n",
    "                        .fit_predict(y_iso_pred.reshape(-1,1), return_centers=True)\n",
    "    # Ensure labels correspond to sorted order of centers\n",
    "    sorted_centers = sorted(enumerate(centers), key=lambda c: c[1])\n",
    "    label_map = {old: new for new, (old, _) in enumerate(sorted_centers)}\n",
    "    labels = np.array([ label_map[l] for l in labels ])\n",
    "    # Identify bin edges where label changes\n",
    "    edges = []\n",
    "    for i in range(1, len(x)):\n",
    "        if labels[i] != labels[i-1]:\n",
    "            edges.append((x[i-1] + x[i]) / 2)\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3399d8a-a288-4289-870c-d8c19f50a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantile_supervised_binning(x, y, n_bins):\n",
    "    # Initial equal-frequency cuts\n",
    "    sorted_idx = np.argsort(x)\n",
    "    x_sorted, y_sorted = x[sorted_idx], y[sorted_idx]\n",
    "    initial_edges = [x_sorted[int(i*len(x)/n_bins)] for i in range(1, n_bins)]\n",
    "    edges = initial_edges[:]\n",
    "    improved = True\n",
    "    while improved:\n",
    "        improved = False\n",
    "        # Check each boundary for potential move\n",
    "        for bi in range(len(edges)):\n",
    "            # compute current MSE\n",
    "            mse_current = compute_mse(x, y, edges)\n",
    "            # Try shifting this edge left or right by one position (if possible) and compute MSE\n",
    "            new_edges_left = adjust_edge_left(edges, bi, x_sorted)\n",
    "            new_edges_right = adjust_edge_right(edges, bi, x_sorted)\n",
    "            # (compute_mse and adjust_edge_left/right are helper functions not shown for brevity)\n",
    "            best_option = min((edges, mse_current), (new_edges_left, mse_left), (new_edges_right, mse_right), key=lambda t: t[1])\n",
    "            if best_option[1] < mse_current:\n",
    "                edges = best_option[0]\n",
    "                improved = True\n",
    "                break  # re-start loop after a successful adjustment\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4380699e-f337-4912-8ef8-eccd13b879b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/soodoku/opt/anaconda3/envs/py311ds/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/soodoku/opt/anaconda3/envs/py311ds/lib/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/Users/soodoku/opt/anaconda3/envs/py311ds/lib/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/Users/soodoku/opt/anaconda3/envs/py311ds/lib/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Bins</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>24.403682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dynamic Programming</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>24.403682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>136.895532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Isotonic Regression</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>195.468801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantile Supervised</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>150.475030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>702.929067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dynamic Programming</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>610.034663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>666.676728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Isotonic Regression</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>611.416474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantile Supervised</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>4413.963329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method       Scenario  Bins          MSE\n",
       "0        Decision Tree  non_monotonic     5    24.403682\n",
       "1  Dynamic Programming  non_monotonic     5    24.403682\n",
       "2              K-Means  non_monotonic     5   136.895532\n",
       "3  Isotonic Regression  non_monotonic     5   195.468801\n",
       "4  Quantile Supervised  non_monotonic     5   150.475030\n",
       "0        Decision Tree      monotonic     5   702.929067\n",
       "1  Dynamic Programming      monotonic     5   610.034663\n",
       "2              K-Means      monotonic     5   666.676728\n",
       "3  Isotonic Regression      monotonic     5   611.416474\n",
       "4  Quantile Supervised      monotonic     5  4413.963329"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Simulate Data ---\n",
    "def simulate_data(n=1000, scenario=\"non_monotonic\"):\n",
    "    np.random.seed(42)\n",
    "    x = np.sort(np.random.uniform(0, 100, n))  # Covariate\n",
    "\n",
    "    if scenario == \"non_monotonic\":\n",
    "        y = np.piecewise(x, [x < 10, (x >= 10) & (x < 40), (x >= 40) & (x < 50), (x >= 50) & (x < 90), x >= 90],\n",
    "                         [50, 20, 40, 10, 30]) + np.random.normal(0, 5, n)\n",
    "    elif scenario == \"monotonic\":\n",
    "        y = 3 * x + np.random.normal(0, 20, n)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown scenario\")\n",
    "\n",
    "    return x, y\n",
    "\n",
    "# --- Decision Tree-Based Binning ---\n",
    "def tree_binning(x, y, n_bins):\n",
    "    tree = DecisionTreeRegressor(max_leaf_nodes=n_bins, random_state=0)\n",
    "    tree.fit(x.reshape(-1, 1), y)\n",
    "    thr = tree.tree_.threshold\n",
    "    edges = sorted(thr[thr > -2])  # Extract split thresholds\n",
    "    return edges\n",
    "\n",
    "# --- Dynamic Programming Optimal Binning ---\n",
    "def optimal_bins_dp(x, y, k):\n",
    "    x, y = np.sort(x), y[np.argsort(x)]\n",
    "    N = len(x)\n",
    "    prefix_sum = np.cumsum(np.concatenate([[0], y]))\n",
    "    prefix_sq = np.cumsum(np.concatenate([[0], y**2]))\n",
    "    dp = np.full((k+1, N+1), np.inf)\n",
    "    prev_cut = np.zeros((k+1, N+1), dtype=int)\n",
    "    dp[0][0] = 0\n",
    "\n",
    "    for i in range(1, N+1):\n",
    "        sum_y, sum_y2 = prefix_sum[i], prefix_sq[i]\n",
    "        seg_len = i\n",
    "        dp[1][i] = sum_y2 - (sum_y**2)/seg_len\n",
    "\n",
    "    for b in range(2, k+1):\n",
    "        for i in range(b, N+1):\n",
    "            for j in range(b-1, i):\n",
    "                sum_y = prefix_sum[i] - prefix_sum[j]\n",
    "                sum_y2 = prefix_sq[i] - prefix_sq[j]\n",
    "                seg_len = i - j\n",
    "                sse_segment = sum_y2 - (sum_y**2)/seg_len\n",
    "                total_sse = dp[b-1][j] + sse_segment\n",
    "                if total_sse < dp[b][i]:\n",
    "                    dp[b][i] = total_sse\n",
    "                    prev_cut[b][i] = j\n",
    "\n",
    "    cuts = []\n",
    "    b, i = k, N\n",
    "    while b > 1:\n",
    "        j = prev_cut[b][i]\n",
    "        cuts.append(j)\n",
    "        i, b = j, b-1\n",
    "    edges = [(x[c-1] + x[c])/2 for c in sorted(cuts)]\n",
    "    return edges\n",
    "\n",
    "# --- K-Means Binning ---\n",
    "def kmeans_binning(x, y, n_bins):\n",
    "    km = KMeans(n_clusters=n_bins, n_init=10, random_state=0)\n",
    "    km.fit(x.reshape(-1, 1))\n",
    "    centers = np.sort(km.cluster_centers_.flatten())\n",
    "    edges = [(centers[i] + centers[i+1]) / 2 for i in range(len(centers)-1)]\n",
    "    return edges\n",
    "\n",
    "# --- Isotonic Regression-Based Binning ---\n",
    "def isotonic_binning(x, y, n_bins):\n",
    "    iso = IsotonicRegression(increasing=True).fit(x, y)\n",
    "    y_iso_pred = iso.predict(x)\n",
    "    km = KMeans(n_clusters=n_bins, n_init=10, random_state=0)\n",
    "    labels = km.fit_predict(y_iso_pred.reshape(-1, 1))\n",
    "    edges = []\n",
    "    for i in range(1, len(x)):\n",
    "        if labels[i] != labels[i-1]:\n",
    "            edges.append((x[i-1] + x[i]) / 2)\n",
    "    return edges\n",
    "\n",
    "# --- Quantile-Based Supervised Binning ---\n",
    "def quantile_supervised_binning(x, y, n_bins):\n",
    "    sorted_idx = np.argsort(x)\n",
    "    x_sorted, y_sorted = x[sorted_idx], y[sorted_idx]\n",
    "    initial_edges = [x_sorted[int(i*len(x)/n_bins)] for i in range(1, n_bins)]\n",
    "    edges = initial_edges[:]\n",
    "    improved = True\n",
    "    while improved:\n",
    "        improved = False\n",
    "        for bi in range(len(edges)):\n",
    "            mse_current = np.mean((y_sorted - np.digitize(x_sorted, edges))**2)\n",
    "            new_edges_left = edges[:]\n",
    "            if bi > 0:\n",
    "                new_edges_left[bi] = (edges[bi-1] + edges[bi]) / 2\n",
    "            mse_left = np.mean((y_sorted - np.digitize(x_sorted, new_edges_left))**2)\n",
    "            new_edges_right = edges[:]\n",
    "            if bi < len(edges)-1:\n",
    "                new_edges_right[bi] = (edges[bi] + edges[bi+1]) / 2\n",
    "            mse_right = np.mean((y_sorted - np.digitize(x_sorted, new_edges_right))**2)\n",
    "            best_option = min((edges, mse_current), (new_edges_left, mse_left), (new_edges_right, mse_right), key=lambda t: t[1])\n",
    "            if best_option[1] < mse_current:\n",
    "                edges = best_option[0]\n",
    "                improved = True\n",
    "                break\n",
    "    return edges\n",
    "\n",
    "# --- Evaluation ---\n",
    "def evaluate_methods(scenario=\"non_monotonic\", n_bins=5):\n",
    "    x, y = simulate_data(scenario=scenario)\n",
    "    methods = {\n",
    "        \"Decision Tree\": tree_binning(x, y, n_bins),\n",
    "        \"Dynamic Programming\": optimal_bins_dp(x, y, n_bins),\n",
    "        \"K-Means\": kmeans_binning(x, y, n_bins),\n",
    "        \"Isotonic Regression\": isotonic_binning(x, y, n_bins),\n",
    "        \"Quantile Supervised\": quantile_supervised_binning(x, y, n_bins),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for method, edges in methods.items():\n",
    "        bin_means = np.zeros_like(y)\n",
    "        bin_indices = np.digitize(x, edges)\n",
    "        for i in range(n_bins):\n",
    "            bin_means[bin_indices == i] = np.mean(y[bin_indices == i])\n",
    "        mse = np.mean((y - bin_means) ** 2)\n",
    "        results.append({\"Method\": method, \"Scenario\": scenario, \"Bins\": n_bins, \"MSE\": mse})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation for both scenarios\n",
    "df_results = pd.concat([evaluate_methods(\"non_monotonic\"), evaluate_methods(\"monotonic\")])\n",
    "df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8375b7a5-1ba4-4833-9112-e6dfd9c5e7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified evaluation function to handle empty bins\n",
    "def evaluate_methods_fixed(scenario=\"non_monotonic\", n_bins=5):\n",
    "    x, y = simulate_data(scenario=scenario)\n",
    "    methods = {\n",
    "        \"Decision Tree\": tree_binning(x, y, n_bins),\n",
    "        \"Dynamic Programming\": optimal_bins_dp(x, y, n_bins),\n",
    "        \"K-Means\": kmeans_binning(x, y, n_bins),\n",
    "        \"Isotonic Regression\": isotonic_binning(x, y, n_bins),\n",
    "        \"Quantile Supervised\": quantile_supervised_binning(x, y, n_bins),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for method, edges in methods.items():\n",
    "        bin_means = np.zeros_like(y)\n",
    "        bin_indices = np.digitize(x, edges)\n",
    "\n",
    "        # Ensure bins are assigned correctly and handle empty bins\n",
    "        for i in range(n_bins):\n",
    "            mask = bin_indices == i\n",
    "            if np.any(mask):  # If bin is not empty\n",
    "                bin_means[mask] = np.mean(y[mask])\n",
    "            else:  # Assign nearest valid bin mean\n",
    "                valid_bins = np.unique(bin_indices)\n",
    "                nearest_bin = min(valid_bins, key=lambda v: abs(v - i))  # Find nearest bin index\n",
    "                bin_means[bin_indices == i] = np.mean(y[bin_indices == nearest_bin])\n",
    "\n",
    "        mse = np.mean((y - bin_means) ** 2)\n",
    "        results.append({\"Method\": method, \"Scenario\": scenario, \"Bins\": n_bins, \"MSE\": mse})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation with fixed handling of empty bins\n",
    "df_results_fixed = pd.concat([\n",
    "    evaluate_methods_fixed(\"non_monotonic\"),\n",
    "    evaluate_methods_fixed(\"monotonic\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2beaa73-28dd-4105-9bac-96bda6ef9660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>363.666374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dynamic Programming</td>\n",
       "      <td>317.219172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Isotonic Regression</td>\n",
       "      <td>403.442637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>401.786130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantile Supervised</td>\n",
       "      <td>2282.219179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method          MSE\n",
       "0        Decision Tree   363.666374\n",
       "1  Dynamic Programming   317.219172\n",
       "2  Isotonic Regression   403.442637\n",
       "3              K-Means   401.786130\n",
       "4  Quantile Supervised  2282.219179"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results_fixed.groupby(\"Method\")[\"MSE\"].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5de2e64-7ad5-4c11-a4f6-6b55742b0dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Scenario</th>\n",
       "      <th>Bins</th>\n",
       "      <th>Huber Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>3.456610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dynamic Programming</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>3.456610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>9.042809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Isotonic Regression</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>10.273906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantile Supervised</td>\n",
       "      <td>non_monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>9.834877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>20.845927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dynamic Programming</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>19.664790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>K-Means</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>20.347413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Isotonic Regression</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>19.742943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Quantile Supervised</td>\n",
       "      <td>monotonic</td>\n",
       "      <td>5</td>\n",
       "      <td>53.599008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Method       Scenario  Bins  Huber Loss\n",
       "0        Decision Tree  non_monotonic     5    3.456610\n",
       "1  Dynamic Programming  non_monotonic     5    3.456610\n",
       "2              K-Means  non_monotonic     5    9.042809\n",
       "3  Isotonic Regression  non_monotonic     5   10.273906\n",
       "4  Quantile Supervised  non_monotonic     5    9.834877\n",
       "0        Decision Tree      monotonic     5   20.845927\n",
       "1  Dynamic Programming      monotonic     5   19.664790\n",
       "2              K-Means      monotonic     5   20.347413\n",
       "3  Isotonic Regression      monotonic     5   19.742943\n",
       "4  Quantile Supervised      monotonic     5   53.599008"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Huber loss function\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    residual = np.abs(y_true - y_pred)\n",
    "    loss = np.where(residual <= delta, 0.5 * residual ** 2, delta * (residual - 0.5 * delta))\n",
    "    return np.mean(loss)\n",
    "\n",
    "# Updated evaluation function with Huber loss optimization\n",
    "def evaluate_methods_huber(scenario=\"non_monotonic\", n_bins=5, delta=1.0):\n",
    "    x, y = simulate_data(scenario=scenario)\n",
    "    methods = {\n",
    "        \"Decision Tree\": tree_binning(x, y, n_bins),\n",
    "        \"Dynamic Programming\": optimal_bins_dp(x, y, n_bins),\n",
    "        \"K-Means\": kmeans_binning(x, y, n_bins),\n",
    "        \"Isotonic Regression\": isotonic_binning(x, y, n_bins),\n",
    "        \"Quantile Supervised\": quantile_supervised_binning(x, y, n_bins),\n",
    "    }\n",
    "    \n",
    "    results = []\n",
    "    for method, edges in methods.items():\n",
    "        bin_means = np.zeros_like(y)\n",
    "        bin_indices = np.digitize(x, edges)\n",
    "\n",
    "        # Ensure bins are assigned correctly and handle empty bins\n",
    "        for i in range(n_bins):\n",
    "            mask = bin_indices == i\n",
    "            if np.any(mask):  # If bin is not empty\n",
    "                bin_means[mask] = np.mean(y[mask])\n",
    "            else:  # Assign nearest valid bin mean\n",
    "                valid_bins = np.unique(bin_indices)\n",
    "                nearest_bin = min(valid_bins, key=lambda v: abs(v - i))  # Find nearest bin index\n",
    "                bin_means[bin_indices == i] = np.mean(y[bin_indices == nearest_bin])\n",
    "\n",
    "        # Compute Huber loss\n",
    "        huber = huber_loss(y, bin_means, delta=delta)\n",
    "        results.append({\"Method\": method, \"Scenario\": scenario, \"Bins\": n_bins, \"Huber Loss\": huber})\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run evaluation with Huber loss optimization\n",
    "df_results_huber = pd.concat([\n",
    "    evaluate_methods_huber(\"non_monotonic\"),\n",
    "    evaluate_methods_huber(\"monotonic\")\n",
    "])\n",
    "\n",
    "df_results_huber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9864b5-f91b-4982-abfd-086f957d478b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Data Science)",
   "language": "python",
   "name": "py311ds"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
